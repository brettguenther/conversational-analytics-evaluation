# Conversational Analytics API Evaluation Framework

This framework is designed to evaluate the performance of Google's Gemini Data Analytics API, specifically for conversational agents built on Looker data models.

## Setup

1.  **Install dependencies:**
    This project uses `uv` for package management. To install the required dependencies, run:
    ```bash
    uv sync
    ```

2.  **Configure Looker API Credentials:**
    Create a `.env` file in the root of the project with your Looker API client ID and secret:
    ```bash
    touch .env
    echo "LOOKER_CLIENT_ID={myClientId}" >> .env
    echo "LOOKER_CLIENT_SECRET={mySecret}" >> .env
    ```

3. **Provide a GCP Auth Token:**
    `gcloud auth application-default login`

## Running the Evaluation

The main entry point for running evaluations is the `main.py` script.

### Example Command

```bash
uv run main.py run-evaluation \
    --project-id="YOUR_PROJECT_ID" \
    --looker-instance="https://your.looker.instance.com" \
    --looker-model="your_looker_model" \
    --looker-explore="your_looker_explore" \
    --config-file=config.json \
    --questions-file=data/questions/questions.json
```

### Command-line Options

-   `--project-id`: Your Google Cloud project ID.
-   `--looker-instance`: The URL of your Looker instance.
-   `--looker-model`: The LookML model to use.
-   `--looker-explore`: The Looker Explore to use.
-   `--config-file`: Path to the `config.json` file.
-   `--questions-file`: Path to the JSON file containing evaluation questions.
-   `--system-instructions-file`: (Optional) Path to a file containing system instructions for the agent.
-   `--agent-id`: (Optional) The ID of the data agent to use. If not provided, a new agent with a unique ID will be created for the run.
-   `--conversation-id`: (Optional) The ID of the conversation to use. If not provided, a new conversation with a unique ID will be created for the run.
-   `--skip-agent-use`: (Optional) A boolean flag that, when present, skips the agent creation and uses the inline context API for a stateless evaluation.

## Evaluation Questions

Evaluation questions are defined in JSON files in the `data/questions/` directory. Each question has the following structure:

```json
{
  "category": "Simple",
  "question": "What were the total sales in the Seattle store in October 2021?",
  "expected_result_text": "The total sales in the Seattle store in October 2021 were $993774.45.",
  "expected_result": [
    {
      "sales.total_sales": 993774.45
    }
  ],
  "reference_query": {
    "model": "ecomm",
    "explore": "sales",
    "fields": ["sales.total_sales"],
    "filters": {
      "sites.site_description": "Downtown Seattle Flagship",
      "sales.calendar_month": "2021-10"
    },
    "limit": "500"
  }
}
```

-   `category`: The category of the question (e.g., "Simple", "Medium", "Hard").
-   `question`: The natural language question to ask the agent.
-   `expected_result_text`: A string representation of the expected result.
-   `expected_result`: A structured representation of the expected result, used to create a pandas DataFrame for comparison.
-   `reference_query`: The Looker query that represents the "golden" answer. Used for the `semantic_correctness` metric.

## Metrics

The framework uses the following metrics to evaluate the agent's responses:

-   **`SQLExactMatch`**: Compares the generated SQL with the expected SQL (if available) for an exact match after normalization.
-   **`DataFrameMatch`**: Compares the dataframe generated by the agent's query with the `expected_result` dataframe.
-   **`semantic_correctness`**: Compares the generated Looker query with the `reference_query`.

## Methodology

TO DO